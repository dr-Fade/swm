@article{WORLD,
  author = {Morise, Masanori and Yokomori, Fumiya and Ozawa, Kenji},
  year = {2016},
  month = {07},
  pages = {1877-1884},
  title = {WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications},
  volume = {E99.D},
  journal = {IEICE Transactions on Information and Systems},
  doi = {10.1587/transinf.2015EDP7457}
}

@inproceedings{DIO,
  title = {Fast and Reliable F0 Estimation Method Based on the Period Extraction of Vocal Fold Vibration of Singing Voice and Speech},
  author = {Masanori Morise and Hideki Kawahara and Haruhiro Katayose},
  year = {2009}
}

@article{CheapTrick,
  title = {CheapTrick, a spectral envelope estimator for high-quality speech synthesis},
  journal = {Speech Communication},
  volume = {67},
  pages = {1-7},
  year = {2015},
  issn = {0167-6393},
  doi = {https://doi.org/10.1016/j.specom.2014.09.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639314000697},
  author = {Masanori Morise},
  keywords = {Speech synthesis, Speech analysis, Spectral envelope, Time-varying component},
  abstract = {A spectral envelope estimation algorithm is presented to achieve high-quality speech synthesis. The concept of the algorithm is to obtain an accurate and temporally stable spectral envelope. The algorithm uses fundamental frequency (F0) and consists of F0-adaptive windowing, smoothing of the power spectrum, and spectral recovery in the quefrency domain. Objective and subjective evaluations were carried out to demonstrate the effectiveness of the proposed algorithm. Results of both evaluations indicated that the proposed algorithm can obtain a temporally stable spectral envelope and synthesize speech with higher sound quality than speech synthesized with other algorithms.}
}

@article{PLATINUM,
  title = {PLATINUM: A method to extract excitation signals for voice synthesis system},
  author = {Masanori Morise},
  journal = {Acoustical Science and Technology},
  volume = {33},
  number = {2},
  pages = {123-125},
  year = {2012},
  doi = {10.1250/ast.33.123}
}

@article{WaveNet,
  author = {A{\"{a}}ron van den Oord and
                  Sander Dieleman and
                  Heiga Zen and
                  Karen Simonyan and
                  Oriol Vinyals and
                  Alex Graves and
                  Nal Kalchbrenner and
                  Andrew W. Senior and
                  Koray Kavukcuoglu},
  title = {WaveNet: {A} Generative Model for Raw Audio},
  journal = {CoRR},
  volume = {abs/1609.03499},
  year = {2016},
  url = {http://arxiv.org/abs/1609.03499},
  eprinttype = {arXiv},
  eprint = {1609.03499},
  timestamp = {Thu, 14 Oct 2021 09:15:04 +0200},
  biburl = {https://dblp.org/rec/journals/corr/OordDZSVGKSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DeepVoice3,
  author = {Wei Ping and
                  Kainan Peng and
                  Andrew Gibiansky and
                  Sercan {\"{O}}mer Arik and
                  Ajay Kannan and
                  Sharan Narang and
                  Jonathan Raiman and
                  John Miller},
  title = {Deep Voice 3: 2000-Speaker Neural Text-to-Speech},
  journal = {CoRR},
  volume = {abs/1710.07654},
  year = {2017},
  url = {http://arxiv.org/abs/1710.07654},
  eprinttype = {arXiv},
  eprint = {1710.07654},
  timestamp = {Tue, 12 Apr 2022 21:46:11 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1710-07654.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{VC1,
  author = {Nguyen Quy Hy and
            Siu Wa Lee and
            Xiaohai Tian and
            Minghui Dong and
            Engsiong Chng},
  title = {High quality voice conversion using prosodic and high-resolution spectral
                  features},
  journal = {CoRR},
  volume = {abs/1512.01809},
  year = {2015},
  url = {http://arxiv.org/abs/1512.01809},
  eprinttype = {arXiv},
  eprint = {1512.01809},
  timestamp = {Mon, 13 Aug 2018 16:46:47 +0200},
  biburl = {https://dblp.org/rec/journals/corr/HyLTDC15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{VC2,
  author = {Sun, Lifa and Kang, Shiyin and Li, Kun and Meng, Helen},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title = {Voice conversion using deep Bidirectional Long Short-Term Memory based Recurrent Neural Networks}, 
  year = {2015},
  volume = {},
  number = {},
  pages = {4869-4873},
  doi = {10.1109/ICASSP.2015.7178896}
}


@InProceedings{STT1,
  title = {Deep Voice: Real-time Neural Text-to-Speech},
  author = {Sercan {\"O}. Ar{\i}k and Mike Chrzanowski and Adam Coates and Gregory Diamos and Andrew Gibiansky and Yongguo Kang and Xian Li and John Miller and Andrew Ng and Jonathan Raiman and Shubho Sengupta and Mohammad Shoeybi},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages = {195--204},
  year = {2017},
  editor = {Precup, Doina and Teh, Yee Whye},
  volume = {70},
  series = {Proceedings of Machine Learning Research},
  month = {06--11 Aug},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v70/arik17a/arik17a.pdf},
  url = {https://proceedings.mlr.press/v70/arik17a.html},
  abstract = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.}
}

@InProceedings{STT2,
  title = {Non-Autoregressive Neural Text-to-Speech},
  author = {Peng, Kainan and Ping, Wei and Song, Zhao and Zhao, Kexin},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages = {7586--7598},
  year = {2020},
  editor = {III, Hal Daumé and Singh, Aarti},
  volume = {119},
  series = {Proceedings of Machine Learning Research},
  month = {13--18 Jul},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v119/peng20a/peng20a.pdf},
  url = {https://proceedings.mlr.press/v119/peng20a.html},
  abstract = {In this work, we propose ParaNet, a non-autoregressive seq2seq model that converts text to spectrogram. It is fully convolutional and brings 46.7 times speed-up over the lightweight Deep Voice 3 at synthesis, while obtaining reasonably good speech quality. ParaNet also produces stable alignment between text and speech on the challenging test sentences by iteratively improving the attention in a layer-by-layer manner. Furthermore, we build the parallel text-to-speech system by applying various parallel neural vocoders, which can synthesize speech from text through a single feed-forward pass. We also explore a novel VAE-based approach to train the inverse autoregressive flow&nbsp;(IAF) based parallel vocoder from scratch, which avoids the need for distillation from a separately trained WaveNet as previous work.}
}

@Inbook{JACK,
  author = {Newmarch, Jan},
  title = {Jack},
  bookTitle = {Linux Sound Programming},
  year = {2017},
  publisher = {Apress},
  address = {Berkeley, CA},
  pages = {143--177},
  isbn = {978-1-4842-2496-0},
  doi = {10.1007/978-1-4842-2496-0\_7},
  url = {https://doi.org/10.1007/978-1-4842-2496-0\_7}
}
@inproceedings{PitchDetection,
  title={Efficient pitch detection techniques for interactive music},
  author={De La Cuadra, Patricio and Master, Aaron S and Sapp, Craig},
  booktitle={ICMC},
  year={2001}
}

@INPROCEEDINGS{PitchDetectionPerformance,
  author={Jouvet, Denis and Laprie, Yves},
  booktitle={2017 25th European Signal Processing Conference (EUSIPCO)}, 
  title={Performance analysis of several pitch detection algorithms on simulated and real noisy speech data}, 
  year={2017},
  volume={},
  number={},
  pages={1614-1618},
  doi={10.23919/EUSIPCO.2017.8081482}
}

@inproceedings{YamahaDIO,
  title={A Fast and Accurate Fundamental Frequency Estimator Using Recursive Moving Average Filters.},
  author={Daido, Ryunosuke and Hisaminato, Yuji},
  booktitle={INTERSPEECH},
  pages={2160--2164},
  year={2016}
}

@software{DSP,
  author       = {Simon Kornblith and
                  Galen Lynch and
                  Martin Holters and
                  João Felipe Santos and
                  Spencer Russell and
                  Jay Kickliter and
                  Jeff Bezanson and
                  Gudmundur Adalsteinsson and
                  Alex Arslan and
                  Ryuichi Yamamoto and
                  jordancluts and
                  Viral B. Shah and
                  Matti Pastell and
                  Tony Kelman and
                  Ben Arthur and
                  Tom Krauss and
                  HDictus and
                  Hamza El-Saawy and
                  Jared Kofron and
                  Eric Hanson and
                  Rob Luke and
                  Aleco Kastanos and
                  Bill and
                  Clemens and
                  Elliot Saba and
                  ibadr and
                  Jake Bolewski and
                  Jordan Smith},
  title        = {JuliaDSP/DSP.jl: v0.7.8},
  month        = dec,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v0.7.8},
  doi          = {10.5281/zenodo.7406426},
  url          = {https://doi.org/10.5281/zenodo.7406426}
}

@Mastersthesis{SpectralEnvelope,
  AUTHOR    = {D. Schwarz},
  TITLE     = "{\emph{Spectral Envelopes in Sound Analysis and Synthesis}}",
  TYPE      = {{Diplomarbeit Nr. 1622}},
  SCHOOL    = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik},
  YEAR      = {1998},
}

@article{ParametricsApplication,
  title={Feature extraction methods LPC, PLP and MFCC in speech recognition},
  author={Dave, Namrata},
  journal={International journal for advance research in engineering and technology},
  volume={1},
  number={6},
  pages={1--4},
  year={2013}
}